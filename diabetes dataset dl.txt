Aim: Performing matrix multiplication and finding eigen vectors and eigen values using TensorFlow.
Code:	import tensorflow as tf
print("Matrix Multiplication Demo")
x=tf.constant([1,2,3,4,5,6],shape=[2,3])
print(x)
y=tf.constant([7,8,9,10,11,12],shape=[3,2])
print(y)
z=tf.matmul(x,y)
print("Product:",z)
e_matrix_A=tf.random.uniform([2,2],minval=3,maxval=10, dtype=tf.float32,name="matrixA")
print("MatrixA:\n{}\n\n".format(e_matrix_A))
eigen_values_A,eigen_vectors_A=tf.linalg.eigh(e_matrix_A)
print("EigenVectors:\n{}\n\nEigenValues:\n{}\n".format(eigen_vectors_A,eigen_values_A))
Output:
Practical No: 2
Aim: Solving XOR problem using deep feed forward network.
Code: import numpy as np
from keras.layers import Dense
from keras.models import Sequential
model = Sequential()
model.add(Dense(units=2, activation='relu', input_dim=2))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
Y = np.array([0., 1., 1., 0.])
model.fit(X, Y, epochs=1000, batch_size=4)
print(model.get_weights())
print(model.predict(X, batch_size=4))
Output:
Practical No:3
Aim: Implementing deep neural network for performing classification task.
Sample data: raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv
Code:from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
dataset=loadtxt('pima-indians-diabetes.csv',delimiter=',')
print(dataset)
X=dataset[:,0:8]
print(X)
Y=dataset[:,8]
print(Y)
model=Sequential()
model.add(Dense(12,input_dim=8,activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(X,Y,epochs=150,batch_size=10)
accuracy=model.evaluate(X,Y)
print('Accuracy of the model is : ',(accuracy*100))
Output:
Practical No: 4
Aim: A) Using deep feed forward network with two hidden layers for perform classification and predicting the class.
Code:from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler
X,Y=make_blobs(n_samples=100,centers=2,n_features=2,random_state=1)
scalar=MinMaxScaler()
scalar.fit(X)
X=scalar.transform(X)
model=Sequential()
model.add(Dense(4,input_dim=2,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam')
model.fit(X,Y,epochs=500)
Xnew,Yreal=make_blobs(n_samples=3,centers=2,n_features=2,random_state=1)
Xnew=scalar.transform(Xnew)
Ynew=model.predict(Xnew)
for i in range(len(Xnew)):
  print("X=%s,Predicted=%s,Desired=%s"%(Xnew[i],Ynew[i],Yreal[i]))
Output:
Aim: B) Using a deep field forward network with two hidden layers for performing classification and predicting the probability of class.
Code:from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler
X,Y=make_blobs(n_samples=100,centers=2,n_features=2,random_state=1)
scalar=MinMaxScaler()
scalar.fit(X)
X=scalar.transform(X)
model=Sequential()
model.add(Dense(4,input_dim=2,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam')
model.fit(X,Y,epochs=500)
Xnew,Yreal=make_blobs(n_samples=3,centers=2,n_features=2,random_state=1)
Xnew=scalar.transform(Xnew)
Yclass=model.predict(Xnew)
Ynew=model.predict_step(Xnew)
for i in range(len(Xnew)):
  print("X=%s,Predicted_probability=%s,Predicted_class=%s"%(Xnew[i],Ynew[i],Yclass[i]))
Output:
Aim: c) Using a deep field forward network with two hidden layers for performing linear regression and predicting values.
Code:from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler
X,Y=make_regression(n_samples=100,n_features=2,noise=0.1,random_state=1)
scalarX,scalarY=MinMaxScaler(),MinMaxScaler()
scalarX.fit(X)
scalarY.fit(Y.reshape(100,1))
X=scalarX.transform(X)
Y=scalarY.transform(Y.reshape(100,1))
model=Sequential()
model.add(Dense(4,input_dim=2,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='mse',optimizer='adam')
model.fit(X,Y,epochs=1000,verbose=0)
Xnew,a=make_regression(n_samples=3,n_features=2,noise=0.1,random_state=1)
Xnew=scalarX.transform(Xnew)
Ynew=model.predict(Xnew)
for i in range(len(Xnew)):
  print("X=%s,Predicted=%s"%(Xnew[i],Ynew[i]))
Output:
Practical No: 5
Aim: Evaluating feed forward deep network for multiclass Classification using KFold cross-validation.
Sample Data: iris.csv
Code:import pandas
from keras.models import Sequential
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier
from keras.utils import to_categorical
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
dataframe = pandas.read_csv("iris.csv", header=None)
dataset = dataframe.values
X = dataset[:, 0:4].astype(float)
Y = dataset[:, 4]
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
dummy_y = to_categorical(encoded_Y)
def baseline_model():
 model = Sequential()
 model.add(Dense(8, input_dim=4, activation='relu'))
 model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
return model
estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
kfold = KFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean() * 100, results.std() * 100))
Output:
Baseline: 96.67% (4.47%)

Practical No:6
Aim: Implementing regularization to avoid overfitting in binary classification.
Code:from matplotlib import pyplot
from sklearn.datasets import make_moons
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l1_l2
X,Y=make_moons(n_samples=100,noise=0.2,random_state=1)
n_train=30
trainX,testX=X[:n_train,:],X[n_train:]
trainY,testY=Y[:n_train],Y[n_train:]
model=Sequential()
model.add(Dense(500,input_dim=2,activation='relu', kernel_regularizer=l1_l2(l1=0.001,l2=0.001)))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
history=model.fit(trainX,trainY,validation_data=(testX,testY),epochs=100)
pyplot.plot(history.history['accuracy'],label='train')
pyplot.plot(history.history['val_accuracy'],label='test')
pyplot.legend()
pyplot.show()
Output:
Practical No:7
Aim: Demonstrate recurrent neural network that learns to perform sequence analysis for stock price.
Sample Data 1: datasets/Google_Stock_Price_Test.csv at master · bananapy/datasets (github.com)
Sample Data 2: stockprice/Google_Stock_Price_Train.csv at master · sudhakarnraju/stockprice (github.com)
Code:  import numpy as np import matplotlib.pyplot as plt
	import pandas as pd  from keras.models import Sequential
	from keras.layers import Dense from keras.layers import LSTM
	from keras.layers import Dropout from sklearn.preprocessing import MinMaxScaler
	dataset_train=pd.read_csv('Google_Stock_Price_Train.csv')
	print(dataset_train) training_set=dataset_train.iloc[:,1:2].values
	print(training_set) sc=MinMaxScaler(feature_range=(0,1))
	training_set_scaled=sc.fit_transform(training_set)
	print(training_set_scaled)
	X_train=[] Y_train=[]
	for i in range(60,1258):
	  X_train.append(training_set_scaled[i-60:i,0])  Y_train.append(training_set_scaled[i,0])
	  X_train,Y_train=np.array(X_train),np.array(Y_train)
	  print(X_train)   print('*********************************')   print(Y_train)
	  X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))
	  print('**********************************************')   print(X_train)
	  regressor=Sequential()
	  regressor.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))
	  regressor.add(Dropout(0.2))   regressor.add(LSTM(units=50,return_sequences=True))
	  regressor.add(Dropout(0.2))   regressor.add(LSTM(units=50,return_sequences=True))
	  regressor.add(Dropout(0.2))   regressor.add(LSTM(units=50))  regressor.add(Dropout(0.2))
	  dataset_test=pd.read_csv('Google_Stock_Price_Test.csv')
	  real_stock_price=dataset_test.iloc[:,1:2].values
	  dataset_total=pd.concat((dataset_train['Open'],dataset_test['Open']),axis=0)
	  inputs=dataset_total[len(dataset_total)-len(dataset_test)-60:].values
	  inputs=inputs.reshape(-1,1)   inputs=sc.transform(inputs)  X_test=[]
	  for i in range(60,80):
	    X_test.append(inputs[i-60:i,0])
	    X_test=np.array(X_test)
	    X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))
	    predicted_stock_price=regressor.predict(X_test)
	    predicted_stock_price=sc.inverse_transform(predicted_stock_price)
	    plt.plot(real_stock_price,color='red',label='real google stock price')
	    plt.plot(predicted_stock_price,color='blue',label='predicted stock price')
	    plt.xlabel('time')     plt.ylabel('google stock price')     plt.legend()     plt.show()
Output:
Practical No: 8
Aim:  Performing encoding and decoding of images using deep autoencoder.
An autoencoder is a neural network architecture designed to learn efficient representations of data. Specifically, it aims to encode input data (such as images) into a compact, lower-dimensional representation (the bottleneck), and then decode this representation back to reconstruct the original data.
Code:    import keras
	from keras import layers from keras.datasets import mnist
	import numpy as np
	encoding_dim=32
	input_img=keras.Input(shape=(784,))
	encoded=layers.Dense(encoding_dim, activation='relu')(input_img)
	decoded=layers.Dense(784, activation='sigmoid')(encoded)
	autoencoder=keras.Model(input_img,decoded)  encoder=keras.Model(input_img,encoded)
	encoded_input=keras.Input(shape=(encoding_dim,))
	decoder_layer=autoencoder.layers[-1]
	decoder=keras.Model(encoded_input,decoder_layer(encoded_input))
	autoencoder.compile(optimizer='adam',loss='binary_crossentropy')
	(X_train,_),(X_test,_)=mnist.load_data()
	X_train=X_train.astype('float32')/255.  X_test=X_test.astype('float32')/255.
	X_train=X_train.reshape((len(X_train),np.prod(X_train.shape[1:])))
	X_test=X_test.reshape((len(X_test),np.prod(X_test.shape[1:])))
	print(X_train.shape) print(X_test.shape)
	autoencoder.fit(X_train,X_train, epochs=50, batch_size=256, shuffle=True, validation_data=(X_test,X_test))
	encoded_imgs=encoder.predict(X_test)
	decoded_imgs=decoder.predict(encoded_imgs)
	import matplotlib.pyplot as plt
	n = 10 # How many digits we will display
	plt.figure(figsize=(40, 4))
	for i in range(10):
	  ax = plt.subplot(3, 20, i + 1)  plt.imshow(X_test[i].reshape(28, 28))
	  plt.gray()   ax.get_xaxis().set_visible(False)   ax.get_yaxis().set_visible(False)
	  ax = plt.subplot(3, 20, i + 1 + 20)  plt.imshow(encoded_imgs[i].reshape(8,4))
	  plt.gray()  ax.get_xaxis().set_visible(False)   ax.get_yaxis().set_visible(False)
	  ax = plt.subplot(3, 20, 2*20 +i+ 1)  plt.imshow(decoded_imgs[i].reshape(28, 28))
	  plt.gray()   ax.get_xaxis().set_visible(False)   ax.get_yaxis().set_visible(False)
	  plt.show()
Output:
Practical No:9
Aim: Implementation of convolutional neural network to predict numbers from images.
Code:  from keras.datasets import mnist
	from keras.utils import to_categorical
	from keras.models import Sequential
	from keras.layers import Dense,Conv2D,Flatten
	import matplotlib.pyplot as plt
	(X_train,Y_train),(X_test,Y_test)=mnist.load_data()
	plt.imshow(X_train[0]) plt.show()
	print(X_train[0].shape)
	X_train=X_train.reshape(60000,28,28,1)
	X_test=X_test.reshape(10000,28,28,1)
	Y_train=to_categorical(Y_train)
	Y_test=to_categorical(Y_test)
	Y_train[0]
	print(Y_train[0])
	model=Sequential()
	model.add(Conv2D(64,kernel_size=3,activation='relu',input_shape=(28,28,1)))
	model.add(Conv2D(32,kernel_size=3,activation='relu'))
	model.add(Flatten())
	model.add(Dense(10,activation='softmax'))
	model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accura cy'])
	#model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=3)
	print(model.predict(X_test[:4]))  print(Y_test[:4])
Output:
Practical No: 10
Aim: Denoising of images using autoencoder.
Code:	import numpy as np import matplotlib.pyplot as plt
	from keras import layers  from keras.datasets import mnist  from keras.models import Model
	(X_train, _), (X_test, _) = mnist.load_data()
	X_train = X_train.astype('float32') / 255. X_test = X_test.astype('float32') / 255.
	X_train = np.reshape(X_train, (len(X_train), 28, 28, 1)) X_test = np.reshape(X_test, (len(X_test), 28, 28, 1))
	noise_factor = 0.5
	X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
	X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
	X_train_noisy = np.clip(X_train_noisy, 0., 1.) X_test_noisy = np.clip(X_test_noisy, 0., 1.)
	n = 10
	plt.figure(figsize=(20, 2))
	for i in range(1, n + 1):
	    ax = plt.subplot(1, n, i)
	    plt.imshow(X_test_noisy[i].reshape(28, 28))
	    plt.gray()    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False) plt.show()
	input_img = layers.Input(shape=(28, 28, 1))
	x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
	x = layers.MaxPooling2D((2, 2), padding='same')(x)
	x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
	encoded = layers.MaxPooling2D((2, 2), padding='same')(x)
	x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
	x = layers.UpSampling2D((2, 2))(x)
	x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
	x = layers.UpSampling2D((2, 2))(x)
	decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
	autoencoder = Model(input_img, decoded)
	autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
	autoencoder.fit(X_train_noisy, X_train, epochs=3, batch_size=128, shuffle=True,
	                validation_data=(X_test_noisy, X_test),
 	               callbacks=[TensorBoard(log_dir='/tmo/tb', histogram_freq=0, write_graph=False)])
	m = 10
	plt.figure(figsize=(20, 2))
	for i in range(1, m + 1):
	    ax = plt.subplot(1, m, i)    plt.imshow(predictions[i].reshape(28, 28))
	    plt.gray()    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False) plt.show()
Output: