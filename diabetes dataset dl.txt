Aim: Install NLTK and perform Text-to-speech and Speech-to-text.  
1a) Install NLTK. 
Install Required Packages: 
• pip install nltk 
• pip install gtts
1b) Convert the given text to speech
code: from gtts import gTTS
    from playsound import playsound
    mytext = "Welcome to SM shetty College"
    language = "en"
    myobj = gTTS(text=mytext, lang=language, slow=False)
    myobj.save("save to dektop/myfile1.mp3")
    playsound("save to dektop/myfile1.mp3")
Output: →check the audio file in folder
1c) Convert audio file speech to text
To be executed in command prompt -
    →pip install SpeechRecognition
    Required file to be downloaded- https://drive.google.com/file/d/1aKzT0WDcXr5tyh19vBNjDI7Aj4S76TGK/view?usp=drive_link
Code
    import speech_recognition as sr
    filename = "audio.wav"
    r = sr.Recognizer()
    with sr.AudioFile(filename) as source:
    audio_data = r.record(source)
    text = r.recognize_google(audio_data)
    print(text)
Output:
2a) Study of various Corpus: Brown, Inaugural, udhr with various methods like fields, raw, words, sents, categories. 
To be executed in command prompt -
	nltk.download('brown')
	nltk.download('inaugural')
	nltk.download('udhr')
Code:
	import nltk
	nltk.download('brown')
	nltk.download('inaugural')
	nltk.download('udhr')
	from nltk.corpus import brown
	print("Brown Corpus\n")
	print("fileids:",brown.fileids())
	print("Categories:",brown.categories())
	print("Sentences:",brown.sents())
	import nltk
	from nltk.corpus import inaugural
	print("Inaugural Corpus:")
	print(inaugural.fileids())
	[fileid[:4] for fileid in inaugural.fileids()]
	import nltk
	from nltk.corpus import udhr
	print("udhr Corpus:")
	print(udhr.fileids())
output:
2b) Create and use your own corpora (plaintext, categorical) 
Steps to be performed -
	Create a folder named “content”
	https://drive.google.com/file/d/1UgGOkosQ83JVxzm3dHyYOdYjLmXA45CL/view?usp=sharing
	download the above file and place it inside the “content” folder.
Code:
	import os, os.path
	path = os.path.expanduser('~/natural_language_toolkit_data')
	if not os.path.exists(path):
	    os.mkdir(path)
	    os.path.exists(path)
	from nltk.corpus.reader import WordListCorpusReader
	reader_corpus= WordListCorpusReader('.', ['desktop/content/wordfile.txt'])
	print(reader_corpus.words())
Output:
2c) Study Conditional frequency distributions
Code:
	text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
	pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]
	import nltk
	from nltk.corpus import brown
	fd = nltk.ConditionalFreqDist((genre, word)
	for genre in brown.categories()
	for word in brown.words(categories=genre))
	genre_word = [(genre, word)
	for genre in ['news', 'romance']
	for word in brown.words(categories=genre)]
	print(len(genre_word)) print(genre_word[:4])
	print(genre_word[-4:])
	cfd = nltk.ConditionalFreqDist(genre_word)
	print(cfd) print(cfd.conditions()) print(cfd['news'])
	print(cfd['romance']) print(list(cfd['romance']))
	from nltk.corpus import inaugural
	cfd = nltk.ConditionalFreqDist((target, fileid[:4])
	for fileid in inaugural.fileids()
	for w in inaugural.words(fileid)
	for target in ['america', 'citizen']
	if w.lower().startswith(target))
	from nltk.corpus import udhr
	languages = ['Chickasaw', 'English', 'German_Deutsch',
	'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
	cfd = nltk.ConditionalFreqDist((lang, len(word))
	for lang in languages
	for word in udhr.words(lang + '-Latin1'))
	cfd.tabulate(conditions=['English', 'German_Deutsch'],
	samples=range(10), cumulative=True)
Output
2d) Study of tagged corpora with methods like tagged_sents, tagged_words.
code:	import nltk
	from nltk import tokenize
	nltk.download('punkt') nltk.download('words')
	para = "Natural language processing (NLP) is a subfield of Artificial Intelligence (AI). This is a widely used technology for personal assistants that are used in various business 	fields/areas. This technology works on the speech provided by the user breaks it down for proper understanding and processes it accordingly."
	sents = tokenize.sent_tokenize(para)
	print("\nsentence tokenization\n=====\n",sents)print("\nword tokenization\n=======\n")
	for index in range(len(sents)): words = tokenize.word_tokenize(sents[index])
    		print(words)
Output:
2e) Write a program to find the most frequent noun tags.
	import nltk
	nltk.download('averaged_perceptron_tagger')
	from collections import defaultdict
	text=nltk.word_tokenize("Nick does like to play football. Nick does not like to play cricket")
	print(text)
	tags=nltk.pos_tag(text)
	addNounWordS=[]
	count=0
	for words in tags: val=tags[count][1]
	    if(val=='NN' or val=='NNS' or val=='NNPS'or val=='NNP'):
	        addNounWordS.append(tags[count][0])
	    count+=1
	print(count) print(addNounWordS)
	temp=defaultdict(int)
	for sub in addNounWordS:
	    for wrd in sub.split(): temp[wrd]+=1
		res=max(temp,key=temp.get)
		print(temp) print("Word with maximum frequency : " + str(res))
output 
2f) Map Words to Properties Using Python Dictionaries.
	thisdict = {"brand": "Ford", "model": "Mustang","year": 1964}
	print(thisdict) print(thisdict["brand"])
	print(len(thisdict)) print(type(thisdict))
output:
Practical No: 3
Aim: Study of wordnet Dictionary 
3a) Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms.
Code:
	import nltk
	nltk.download('wordnet')
	from nltk.corpus import wordnet
	try:
	    syn = wordnet.synsets('game')[0]
	    print ("Synset name : ", syn.name())
	    print ("\nSynset meaning : ", syn.definition())    
	    print ("\nSynset example : ", syn.examples())     
	except IndexError:
	    print('word not found!')
output:
3b) Study lemmas, hyponyms, hypernyms
	import nltk
	from nltk.corpus import wordnet
	nltk.download('wordnet')
	syn = wordnet.synsets('hello')[0] print ("Syn tag for hello: ", syn.pos())
	syn = wordnet.synsets('doing')[0] print ("Syn tag for doing: ", syn.pos())
	syn = wordnet.synsets('beautiful')[0] print ("Syn tag for beauiful: ", syn.pos())
	syn = wordnet.synsets('quickly')[0] print ("Syn tag for quickly: ", syn.pos())
Output:
3c) Write a program using python to find synonym and antonym of word "active" using Wordnet.
code:	import nltk
	from nltk.corpus import wordnet
	print( wordnet.synsets("active"))  print(wordnet.lemma('active.a.01.active').antonyms())
output:
3d) Compare two nouns
code:	import nltk 
	from nltk.corpus import wordnet 
	syn1 = wordnet.synsets(computer) 
	syn2 = wordnet.synsets(calculator) 
	for s1 in syn1: 
	for s2 in syn2: 
	print("Path similarity of: ") 
	print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']') 
	print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']') 
	print(" is", s1.path_similarity(s2)) 
	print()
3e) Handling stopword. 
i. Using nltk Adding or Removing Stop Words in NLTK's Default Stop Word List. 
Code:    import nltk
	from nltk.corpus import stopwords
	nltk.download('stopwords')
	from nltk.tokenize import word_tokenize
	text = "Yashesh likes to play football, however he is not too fond of tennis."
	text_tokens = word_tokenize(text)
	tokens_without_sw = [word for word in text_tokens if not word in
	stopwords.words()]
	print(tokens_without_sw)
	#add the word play to the NLTK stop word collection
	all_stopwords = stopwords.words('english')
	all_stopwords.append('play')
	text_tokens = word_tokenize(text)
	tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]
	print(tokens_without_sw)
	#remove ‘not’ from stop word collection
	all_stopwords.remove('not')
	text_tokens = word_tokenize(text)
	tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]
	print(tokens_without_sw)
Output:
ii. Using Gensim Adding and Removing Stop Words in Default Gensim Stop Words List. 
Code:	import nltk
	from nltk.tokenize import word_tokenize
	import gensim
	from gensim.parsing.preprocessing import remove_stopwords
	text = "Yashesh likes to play football, however he is not too fond of tennis."
	filtered_sentence = remove_stopwords(text)
	print(filtered_sentence)
	all_stopwords = gensim.parsing.preprocessing.STOPWORDS
	print(all_stopwords)
	'''The following script adds likes and play to the list of stop words in Gensim:'''
	from gensim.parsing.preprocessing import STOPWORDS
	all_stopwords_gensim = STOPWORDS.union(set(['likes', 'play']))
	text = "Yashesh likes to play football, however he is not too fond of tennis."
	text_tokens = word_tokenize(text)
	tokens_without_sw = [word for word in text_tokens if not word in
	all_stopwords_gensim]
	print(tokens_without_sw)
Output:
Practical No: 4
Aim: Performing Text Tokenization using various functions and libraries. 
4a) Tokenization using python split function.
code: 	text = """ This tool is a beta stage. Alexa developers can use Get Metrics API to seamlessly analyse metric. It also supports custom skill model, prebuilt Flash Briefing model, and 	the Smart Home Skill API. You can use this tool for creation of monitors, alarms, and dashboards that spotlight changes. The release of these three tools will enable developers to 	create visual rich skills for Alexa devices with screens. Amazon describes these tools as the collection of tech and tools for creating visually rich and interactive voice 	experiences. """
	data = text.split('.')
	for i in data:
	   	 print (i)
output:
4b) Tokenization using python regular expression (RegEx).
code:	import nltk
	from nltk.tokenize import RegexpTokenizer
	tk = RegexpTokenizer('\s+', gaps = True)
	str = "I love to study Natural Language Processing in Python"
	tokens = tk.tokenize(str)
	print(tokens)
output:
4c) Tokenization using NLTK
code:	import nltk
	from nltk.tokenize import word_tokenize
	str = "I love to study Natural Language Processing in Python"
	print(word_tokenize(str))
4d) Tokenization using the spacy library.
Command to execute in cmd:
	pip install spacy
Code:
	import spacy
	nlp = spacy.blank("en")
	str = "I love to study Natural Language Processing in Python"
	doc = nlp(str)
	words = [word.text for word in doc]
	print(words)
4e) Tokenization using Keras
Command to execute in cmd:
	pip install keras
	pip install tensorflow
Code:
	from tensorflow.keras.preprocessing.text import text_to_word_sequence
	text="I love to study Natural Language Processing in Python"
	result=text_to_word_sequence(text)
	print(result)
4f) Tokenization using Gensim.
Command to execute in cmd:
	pip install gensim
Code:
	import nltk
	import gensim
	from gensim.utils import tokenize
	str="I love to study Natural Language Processing in python"
	print(list(tokenize(str)))
Practical No: 5
Aim: Import NLP Libraries for Indian Languages 
command to be executed on cmd
	pip install inltk
	pip install scikit-learn
Code:
	from inltk.inltk import setup
	from inltk.inltk import tokenizesetup(“hi”)  
	hindi_text = """प्राकृतिक भाषा सीखना बहुत तिलचस्प है।"""
	# tokenize(input text, language code)
	print(tokenize(hindi_text, "hi"))
output:
Practical No: 6
Aim: Illustrate part of speech tagging. 
6a) Part of speech Tagging and chunking of user defined text. 
Code:   import nltk
	from nltk import tokenize
	from nltk import tag
	from nltk import chunk nltk.download('punkt')
	nltk.download('averaged_perceptron_tagger')
	nltk.download('maxent_ne_chunker')
	nltk.download('words')
	para = "Hello! My name is Rakesh Mandal. Today you'll be learning NLTK."
	sents = tokenize.sent_tokenize(para)
	print("\nsentence tokenization\n==============") print(sents)
	print("\nword tokenization\n=================")
	for index in range(len(sents)): words = tokenize.word_tokenize(sents[index])
	    print(words)
	tagged_words = []
	for index in range(len(sents)):
	    tagged_words.append(tag.pos_tag(tokenize.word_tokenize(sents[index])))
	print("\nPOS Tagging\n===========")  print(tagged_words)
	tree = []
	for index in range(len(sents)): tree.append(chunk.ne_chunk(tagged_words[index]))
	print("\nChunking\n========")
	for t in tree: print(t)
Output:
6b) Named Entity recognition using user defined text.
command to run on cmd:
	1)	pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu
(Reference url: https://discuss.pytorch.org/t/error-loading-lib-site-packages-torch-lib-shm-dll-or-one-of-its-dependencie/201695)
	2)	python -m spacy download en_core_web_sm
code:
	import spacy
	nlp = spacy.load("en_core_web_sm")
	text = ("When Sebastian Thrun started working on self-driving cars at "
	"Google in 2007, few people outside of the company took him "
	"seriously. “I can tell you very senior CEOs of major American "
	"car companies would shake my hand and turn away because I wasn’t "
	"worth talking to,” said Thrun, in an interview with Recode earlier "
	"this week.")
	doc = nlp(text)
	print("Noun phrases:", [chunk.text for chunk in doc.noun_chunks])
	print("Verbs:", [token.lemma_ for token in doc if token.pos_ == "VERB"])
Output:
6c) Named Entity recognition with diagram using NLTK corpus – treebank 
Code:
	import nltk
	nltk.download('treebank')
	from nltk.corpus import treebank_chunk
	treebank_chunk.tagged_sents()[0]
	treebank_chunk.chunked_sents()[0]
	treebank_chunk.chunked_sents()[0].draw()
Output
Practical No: 7
Aim: Finite state automata 
7a) Define grammar using nltk. Analyze a sentence using the same 
Code:  import nltk
	from nltk import tokenize
	grammar1 = nltk.CFG.fromstring("""
	S -> VP
	VP -> VP NP
	NP -> Det NP
	Det -> 'that'
	NP -> singular Noun
	NP -> 'flight'
	VP -> 'Book' """)
	sentence = "Book that flight"
	for index in range(len(sentence)):
	all_tokens = tokenize.word_tokenize(sentence)
	print(all_tokens)
	parser = nltk.ChartParser(grammar1)
	for tree in parser.parse(all_tokens):
	print(tree)
	tree.draw()
Output:
7b) Accept the input string with Regular expression of Finite Automaton: 101+ 
Code:  def FA(s):
    if len(s)<3:
        return "Rejected"
    if s[0]=='1':
        if s[1]=='0':
            if s[2]=='1':
                for i in range(3,len(s)):
                    if s[i]!='1':
                        return "Rejected"
                    return "Accepted"
                return "Rejected"
            return "Rejected"
        return "Rejected"
inputs=['1','10101','101','10111','101101',""]
for i in inputs:
    print(FA(i))
Output:
7c) Accept the input string with Regular expression of FA: (a+b)*bba 
Code:  def FA(s):
    size=0
    for i in s:
        if i=='a' or i=='b':
            size+=1
        else:
            return "Rejected"
    if size>=3:
        if s[size-3]=='b':
            if s[size-2]=='b':
                if s[size-1]=='a':
                    return "Accepted"
                return "Rejected"
            return "Rejected"
        return "Rejected"
    return "Rejected"
inputs=['bba', 'ababbba', 'abba','abb', 'baba','bbb','']
for i in inputs:
    print(FA(i))
Output: 
d) Implementation of Deductive Chart Parsing using context free grammar and a given sentence Code:   import nltk
code:	from nltk import tokenize
	grammar1 = nltk.CFG.fromstring("""
	S -> NP VP
	PP -> P NP
	NP -> Det N | Det N PP | 'I'
	VP -> V NP | VP PP
	Det -> 'a' | 'my'
	N -> 'bird' | 'balcony'
	V -> 'saw'
	P -> 'in'
	""")
	sentence = "I saw a bird in my balcony"
	for index in range(len(sentence)):
	    all_tokens = tokenize.word_tokenize(sentence)
	print(all_tokens)
	# all_tokens = ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']
	parser = nltk.ChartParser(grammar1)
	for tree in parser.parse(all_tokens):
	    print(tree)
	    tree.draw()
Output
Practical No: 8
Aim: Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer and Study WordNetLemmatizer 
8a) PorterStemmer 
The Porter stemming algorithm is a widely used algorithm for stemming English words, which reduces each word to its base or root form. It was developed by Martin Porter in 1979 and has since become a popular algorithm for text processing tasks like information retrieval, information extraction, and text classification.
Code:  import nltk
	from nltk.stem import PorterStemmer
	word_stemmer = PorterStemmer()
	print(word_stemmer.stem('writing'))
Output:
8b) LancasterStemmer 
The Lancaster stemming algorithm is another widely used algorithm for stemming English words, which is similar to the Porter stemming algorithm but often produces more aggressive stemming. It was developed by Chris Paice in 1990 and is known for its speed and simplicity. 
Code:   import nltk
	from nltk.stem import LancasterStemmer
	Lanc_stemmer = LancasterStemmer()
	print(Lanc_stemmer.stem('writing'))
Output:
c) RegexpStemmer 
The RegexpStemmer is a stemmer in nltk library that allows to specify a regular expression pattern to match against words and remove matching suffixes to obtain the stem of the word. This allows for more customizable stemming than the Porter or Lancaster stemmers. 
Code:    import nltk
	from nltk.stem import RegexpStemmer
	Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
	print(Reg_stemmer.stem('writing'))
Output: 
d) SnowballStemmer 
The Snowball stemming algorithm, also known as the Porter2 stemming algorithm, is an improvement over the original Porter stemming algorithm that has been extended to support multiple languages. It was developed by Martin Porter and is available in the nltk library as the SnowballStemmer. 
Code:    import nltk
	from nltk.stem import SnowballStemmer
	english_stemmer = SnowballStemmer('english')
	print(english_stemmer.stem ('writing'))
Output:
8e) WordNetLemmatizer 
The WordNetLemmatizer is a lemmatizer in the nltk library that uses WordNet to find the base or dictionary form of a word. Unlike a stemmer, a lemmatizer attempts to return a valid word that can be found in a dictionary.
Code: import nltk
	from nltk.stem import WordNetLemmatizer
	lemmatizer = WordNetLemmatizer()
	print("word :\tlemma")
	print("rocks :", lemmatizer.lemmatize("rocks"))
	print("corpora :", lemmatizer.lemmatize("corpora"))
	print("better :", lemmatizer.lemmatize("better", pos ="a"))
Output:
